{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65428ab3-7d44-4c1e-befd-923ef2d7a6eb",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44ca8a0-fc83-48d1-9207-df3d3acb01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba487a65-9aad-4c61-a5e8-751062864a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99908bf-588c-405d-bca2-3573afb6dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from sort.sort import Sort\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a293860b-8e54-4c4e-ae6e-ab21d86c9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrOCRProcessor\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18adf755-68e9-492c-a6f6-faa3f934ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\Screenshot 2025-07-08 162015.png: 384x640 19 cars, 3 trucks, 83.1ms\n",
      "Speed: 6.5ms preprocess, 83.1ms inference, 193.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained YOLOv8 model\n",
    "model = YOLO('yolov8n.pt') \n",
    "\n",
    "# Testing on an image\n",
    "results = model(\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\Screenshot 2025-07-08 162015.png\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf00c73e-8846-4831-8c98-af6fbf2b9a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 37.8ms\n",
      "Speed: 6.0ms preprocess, 37.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 37.4ms\n",
      "Speed: 5.0ms preprocess, 37.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 37.5ms\n",
      "Speed: 4.5ms preprocess, 37.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 37.2ms\n",
      "Speed: 4.9ms preprocess, 37.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "video_path = \"data/your_video.mp4\"\n",
    "cap = cv2.VideoCapture(r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\videos\\2103099-uhd_3840_2160_30fps.mp4\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Running YOLOv8 detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Plotting results on frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Displaying the frame\n",
    "    display_frame = cv2.resize(annotated_frame, (960, 540))\n",
    "    cv2.imshow(\"YOLOv8 Detection\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "700a7f4d-37cf-4a59-a950-625b5c7c9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 37.6ms\n",
      "Speed: 5.8ms preprocess, 37.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 37.2ms\n",
      "Speed: 4.9ms preprocess, 37.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 37.5ms\n",
      "Speed: 4.5ms preprocess, 37.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 36.9ms\n",
      "Speed: 4.7ms preprocess, 36.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 36.8ms\n",
      "Speed: 4.7ms preprocess, 36.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 36.8ms\n",
      "Speed: 4.3ms preprocess, 36.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "tracker = Sort()\n",
    "\n",
    "cap = cv2.VideoCapture(r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\videos\\2103099-uhd_3840_2160_30fps.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "\n",
    "    # Parseing detections for SORT: (x1,y1,x2,y2,score)\n",
    "    dets = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            dets.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "    dets = np.array(dets)\n",
    "    \n",
    "    tracks = tracker.update(dets)\n",
    "\n",
    "    # Drawing tracked objects\n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2, track_id = track\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f'ID: {int(track_id)}', (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "    display_frame = cv2.resize(frame, (960, 540))\n",
    "    cv2.imshow(\"YOLOv8 + SORT Tracking\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75ea57-3614-4ef9-9781-6a34bca5e2ab",
   "metadata": {},
   "source": [
    "Speed Estimation\n",
    "- assumptions :\n",
    "  - fixed camera angle (no moving camera)\n",
    "  - Speed is estimated relatively unless calibrated with real-world scale (e.g. distance per pixel)\n",
    "- Implementation Strategy :\n",
    "  - Storing previous positions of each tracked ID\n",
    "  - Calculating pixel distance moved per frame\n",
    "  - Convert to real-world speed using:\n",
    "     - Speed (km/h) = pixel distance × scale factor × fps × 3.6 / 1000\n",
    "  - scale factor: meters per pixel (needs calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54983b21-c6e9-4add-8c79-c1a31d640068",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_plate_detection_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tracker = Sort()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plate_model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath_to_plate_detection_model.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ocr_reader = easyocr.Reader([\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m cap = cv2.VideoCapture(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mprojects\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mAutomated Vehicle Speed & Number Plate Detection System\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mvideos\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2103099-uhd_3840_2160_30fps.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:79\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:151\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:295\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    292\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1549\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1535\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattempt_load_one_weight\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1536\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[33;03m    Load a single model weights.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1547\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1548\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1550\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1551\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1447\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1445\u001b[39m                 ckpt = torch_load(f, pickle_module=safe_pickle)\n\u001b[32m   1446\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m             ckpt = \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\ultralytics\\utils\\patches.py:118\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    116\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\plate_detection_env\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'path_to_plate_detection_model.pt'"
     ]
    }
   ],
   "source": [
    "tracker = Sort()\n",
    "plate_model = YOLO(\"path_to_plate_detection_model.pt\")\n",
    "ocr_reader = easyocr.Reader(['en'])\n",
    "\n",
    "cap = cv2.VideoCapture(r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\videos\\2103099-uhd_3840_2160_30fps.mp4\")\n",
    "\n",
    "prev_positions = {}\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30  # fallback\n",
    "meters_per_pixel = 0.05  # example scale factor\n",
    "speed_history = {}  # To store recent speeds per track\n",
    "history_len = 5     # Number of frames to average over\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "\n",
    "    # Parsing detections for SORT: (x1,y1,x2,y2,score)\n",
    "    dets = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            dets.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "    dets = np.array(dets)\n",
    "    \n",
    "    tracks = tracker.update(dets)\n",
    "\n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2, track_id = track\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        \n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        \n",
    "        speed = 0.0\n",
    "        \n",
    "        if track_id in prev_positions:\n",
    "            prev_x, prev_y = prev_positions[track_id]\n",
    "            pixel_distance = np.sqrt((center_x - prev_x)**2 + (center_y - prev_y)**2)\n",
    "            instant_speed = pixel_distance * meters_per_pixel * fps * 3.6\n",
    "            \n",
    "            # Update speed history\n",
    "            if track_id not in speed_history:\n",
    "                speed_history[track_id] = []\n",
    "            speed_history[track_id].append(instant_speed)\n",
    "            \n",
    "            # Keep only recent N values\n",
    "            if len(speed_history[track_id]) > history_len:\n",
    "                speed_history[track_id].pop(0)\n",
    "            \n",
    "            # Calculate average speed\n",
    "            speed = np.mean(speed_history[track_id])\n",
    "        \n",
    "        prev_positions[track_id] = (center_x, center_y)\n",
    "        \n",
    "        # Draw bounding box, ID, and smoothed speed\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f'ID: {int(track_id)}', (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "        cv2.putText(frame, f'Speed: {speed:.1f} km/h', (x1, y2+20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "    \n",
    "        \n",
    "        # Updating previous position\n",
    "        prev_positions[track_id] = (center_x, center_y)\n",
    "        \n",
    "        # Drawing bounding box and ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f'ID: {int(track_id)}', (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "        # Cropping vehicle region\n",
    "        vehicle_crop = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Detecting number plate within vehicle\n",
    "        results = plate_model(vehicle_crop)\n",
    "        plates = results[0].boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        for plate in plates:\n",
    "            px1, py1, px2, py2 = map(int, plate)\n",
    "            plate_crop = vehicle_crop[py1:py2, px1:px2]\n",
    "            \n",
    "            # OCR\n",
    "            ocr_result = ocr_reader.readtext(plate_crop)\n",
    "            plate_text = ocr_result[0][1] if ocr_result else \"Unknown\"\n",
    "            \n",
    "            # Draw plate box and text\n",
    "            cv2.rectangle(vehicle_crop, (px1, py1), (px2, py2), (0,0,255), 2)\n",
    "            cv2.putText(frame, f'Plate: {plate_text}', (x1, y2+40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "\n",
    "    display_frame = cv2.resize(frame, (960, 540))\n",
    "    cv2.imshow(\"YOLOv8 + SORT Tracking\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9ea74-a5c4-4ea5-b65e-9ce2f49b2cae",
   "metadata": {},
   "source": [
    "Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0979ecad-870c-4d4c-8e79-45773910731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entries:\n",
      "    image_name info    label\n",
      "0  ak1165.png   ak   FUW999\n",
      "1   ak399.png   ak   FGJ235\n",
      "2   ak721.png   ak   FHG521\n",
      "3   ak848.png   ak   FPJ331\n",
      "4  al1156.png   al  6A5730A\n",
      "All images found\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\groundtruth.csv\"\n",
    "images_dir = r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\usimages\"\n",
    "\n",
    "df = pd.read_csv(csv_path, header=None, names=['image_name', 'info', 'label'])\n",
    "\n",
    "print(\"Sample entries:\\n\", df.head())\n",
    "\n",
    "# Checking missing images\n",
    "missing = []\n",
    "for img_name in df['image_name']:\n",
    "    img_path = os.path.join(images_dir, img_name)\n",
    "    if not os.path.exists(img_path):\n",
    "        missing.append(img_name)\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing {len(missing)} images:\", missing[:5])\n",
    "else:\n",
    "    print(\"All images found\")\n",
    "\n",
    "samples = df.sample(5)\n",
    "for idx, row in samples.iterrows():\n",
    "    img_path = os.path.join(images_dir, row['image_name'])\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {row['label']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0404f-8d14-462c-9d72-41f3eae7a510",
   "metadata": {},
   "source": [
    "EasyOCR Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963e7dce-200a-4c2c-8952-134b1c525d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: XSTASY, Prediction: ['WISCONSIN', 'XSTASY', \"America's Dairyland\"]\n",
      "GT: BJL833, Prediction: [\"'GEORGIA:gov\", 'BJL6833', 'TDEC', 'JACKSON']\n",
      "GT: 845CTC, Prediction: ['EXPLORE', 'Minnesota comn', '845-CTC', 'JUD -', '10.000 lakes', '10']\n",
      "GT: ZT714, Prediction: ['Rhode Island -', 'ZT-714', 'Ocean State', '@cU']\n",
      "GT: 47A8D4C, Prediction: ['Stars Fell On', '47A8D4C', 'Nov]', 'Alabama', '20062']\n",
      "GT: 326XLH, Prediction: ['IowA', '326 XLH', 'POTTAWATTAMIE']\n",
      "GT: PLTFRM, Prediction: ['Sllinois', 'PLT FRM']\n",
      "GT: 0JW695, Prediction: ['Webrska', 'I1o}', 'OJK 695']\n",
      "GT: MNW235, Prediction: ['SuncIDAHO', 'MNR 235', 'FAMOUS POTATOE', '08 !']\n",
      "GT: FAE6137, Prediction: ['NEW YORK', 'FAE.6137', 'EMPIRE STATE']\n",
      "EasyOCR baseline accuracy on 10 samples: 20.00%\n"
     ]
    }
   ],
   "source": [
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Evaluating on a few samples\n",
    "correct = 0\n",
    "total = 10  \n",
    "\n",
    "samples = df.sample(total)\n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    img_path = os.path.join(images_dir, row['image_name'])\n",
    "    \n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    result = reader.readtext(img_gray, detail=0)\n",
    "    \n",
    "    print(f\"GT: {row['label']}, Prediction: {result}\")\n",
    "    \n",
    "    # Check if GT label is in prediction list (basic exact match logic)\n",
    "    if any(row['label'] in pred for pred in result):\n",
    "        correct += 1\n",
    "\n",
    "print(f\"EasyOCR baseline accuracy on {total} samples: {correct/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e9ceb-dbac-4d8a-9467-981b49fdb4ae",
   "metadata": {},
   "source": [
    "TrOCR Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77d9471-0637-4814-b96a-931ce7487685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/projects/Automated Vehicle Speed & Number Plate Detection System/data/groundtruth.csv', names=['image_name', 'info', 'label'])\n",
    "\n",
    "df = df[['image_name', 'label']]\n",
    "\n",
    "# Adding full image path column\n",
    "images_dir = 'D:/projects/Automated Vehicle Speed & Number Plate Detection System/data/usimages'\n",
    "df['image_path'] = df['image_name'].apply(lambda x: os.path.join(images_dir, x))\n",
    "\n",
    "# Converting to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa17258-843d-4340-a03f-d759273c468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc19974-3e36-463b-b0f7-7482aa760c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = processor.tokenizer(examples['label'], padding=\"max_length\", max_length=16, truncation=True).input_ids\n",
    "    return {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e810fd7c-53fd-4355-afc3-a621f5b4edff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████| 751/751 [00:13<00:00, 55.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, batched=True, remove_columns=['image_name', 'image_path', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7fce64-421a-4d2d-aa05-b86f61b6d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab7c17a-4041-4e37-8c83-a9f2d97f50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f343efc-bca4-469b-930b-d25c55a4bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 2:02:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.783500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.165300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.5393171388629481, metrics={'train_runtime': 7345.8243, 'train_samples_per_second': 0.307, 'train_steps_per_second': 0.077, 'total_flos': 1.685886535313916e+18, 'train_loss': 0.5393171388629481, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr_numberplate_finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28cc3277-ce00-4e86-8bce-15a246d1ae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./trocr_numberplate_finetuned')\n",
    "processor.save_pretrained('./trocr_numberplate_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba8e78e2-3f1c-4386-823d-d547db51192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: UAA47C, Prediction: UU47\n",
      "GT: BT000, Prediction: BT\n",
      "GT: 615KPI, Prediction: 615PIPI\n",
      "GT: 547KPE, Prediction: 555547PE\n",
      "GT: 0, Prediction: 0\n",
      "GT: 704BBJ, Prediction: 704BB\n",
      "GT: G0LFING, Prediction: GG0LING\n",
      "GT: KAB277, Prediction: KK277\n",
      "GT: DIS6847, Prediction: DIS47\n",
      "GT: 299YX, Prediction: 299X\n",
      "TrOCR fine-tuned model accuracy on 10 samples: 10.00%\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loading fine-tuned model\n",
    "model = VisionEncoderDecoderModel.from_pretrained('./trocr_numberplate_finetuned').to(device)\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "\n",
    "# Evaluating on 10 random samples\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 10\n",
    "\n",
    "samples = df.sample(total)\n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    img_path = os.path.join(images_dir, row['image_name'])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Prediction\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    prediction = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"GT: {row['label']}, Prediction: {prediction}\")\n",
    "    \n",
    "    # Checking if prediction matches GT\n",
    "    if row['label'] in prediction.replace(\" \", \"\").replace(\"-\", \"\"):\n",
    "        correct += 1\n",
    "\n",
    "print(f\"TrOCR fine-tuned model accuracy on {total} samples: {correct/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd7fe1a-eccd-4d65-8475-e68a2eb07282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 696PAI, Prediction: 696PA\n",
      "GT: 696PAI, Prediction: 696PA, Similarity: 91%\n",
      "GT: A000956, Prediction: AA956\n",
      "GT: A000956, Prediction: AA956, Similarity: 67%\n",
      "GT: NRM331, Prediction: NR331\n",
      "GT: NRM331, Prediction: NR331, Similarity: 91%\n",
      "GT: 713731, Prediction: 7137\n",
      "GT: 713731, Prediction: 7137, Similarity: 80%\n",
      "GT: W17K, Prediction: WWK\n",
      "GT: W17K, Prediction: WWK, Similarity: 57%\n",
      "GT: CX4887, Prediction: CC48\n",
      "GT: CX4887, Prediction: CC48, Similarity: 60%\n",
      "GT: HNB773, Prediction: HH773\n",
      "GT: HNB773, Prediction: HH773, Similarity: 73%\n",
      "GT: 1410502, Prediction: 14502\n",
      "GT: 1410502, Prediction: 14502, Similarity: 83%\n",
      "GT: 5415791, Prediction: 5415191\n",
      "GT: 5415791, Prediction: 5415191, Similarity: 86%\n",
      "GT: 1WG011, Prediction: 11WG\n",
      "GT: 1WG011, Prediction: 11WG, Similarity: 60%\n",
      "TrOCR fine-tuned model accuracy on 10 samples: 50.00%\n",
      "Fuzzy match accuracy on 10 samples: 50.00%\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "threshold = 80  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loading fine-tuned model\n",
    "model = VisionEncoderDecoderModel.from_pretrained('./trocr_numberplate_finetuned').to(device)\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "\n",
    "# Evaluating on 10 random samples\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 10\n",
    "\n",
    "samples = df.sample(total)\n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    img_path = os.path.join(images_dir, row['image_name'])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Prediction\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    prediction = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"GT: {row['label']}, Prediction: {prediction}\")\n",
    "    \n",
    "    # Checking if prediction matches GT\n",
    "    if row['label'] in prediction.replace(\" \", \"\").replace(\"-\", \"\"):\n",
    "        correct += 1\n",
    "\n",
    "    score = fuzz.ratio(row['label'], prediction.replace(\" \", \"\").replace(\"-\", \"\"))\n",
    "    print(f\"GT: {row['label']}, Prediction: {prediction}, Similarity: {score}%\")\n",
    "    \n",
    "    if score >= threshold:\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"TrOCR fine-tuned model accuracy on {total} samples: {correct/total*100:.2f}%\")\n",
    "print(f\"Fuzzy match accuracy on {total} samples: {correct/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0511923-b82a-47ef-b086-d4e9438afc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number plate: FU999\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('./trocr_numberplate_finetuned')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('./trocr_numberplate_finetuned').to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def read_number_plate(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "sample_img = \"D:/projects/Automated Vehicle Speed & Number Plate Detection System/data/usimages/ak1165.png\"\n",
    "\n",
    "predicted_plate = read_number_plate(sample_img)\n",
    "print(\"Predicted number plate:\", predicted_plate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced2ea2-6820-4f83-b3c3-def120915936",
   "metadata": {},
   "source": [
    "Dataset for number plate detection fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89adea82-3395-4c20-8297-2fa7d4c6c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to YOLO format completed\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\"\n",
    "output_labels_dir = os.path.join(data_dir, \"labels_yolo\")\n",
    "os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "# Process each txt label file\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        txt_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        with open(txt_path, \"r\") as f:\n",
    "            line = f.readline().strip()\n",
    "            parts = line.split(\"\\t\")\n",
    "            img_name, x, y, w, h, plate = parts\n",
    "            \n",
    "            # Read image to get dimensions\n",
    "            img_path = os.path.join(data_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            img_h, img_w = img.shape[:2]\n",
    "            \n",
    "            # Converting to YOLO format\n",
    "            x, y, w, h = map(float, [x, y, w, h])\n",
    "            x_center = (x + w/2) / img_w\n",
    "            y_center = (y + h/2) / img_h\n",
    "            w_norm = w / img_w\n",
    "            h_norm = h / img_h\n",
    "            \n",
    "            yolo_line = f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\"\n",
    "            \n",
    "            # Writing to new label file (same filename but in output dir)\n",
    "            out_label_path = os.path.join(output_labels_dir, file)\n",
    "            with open(out_label_path, \"w\") as out_f:\n",
    "                out_f.write(yolo_line)\n",
    "\n",
    "print(\"Conversion to YOLO format completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b61e93c7-6109-4ec5-b0bc-d8fe6334f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88a11012-15c0-4aba-98bf-70201c4bd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset organized: 199 train images, 23 val images\n"
     ]
    }
   ],
   "source": [
    "labels_dir = os.path.join(data_dir, \"labels_yolo\")\n",
    "\n",
    "output_base = os.path.join(data_dir, \"yolo_dataset\")\n",
    "images_train_dir = os.path.join(output_base, \"images\", \"train\")\n",
    "images_val_dir = os.path.join(output_base, \"images\", \"val\")\n",
    "labels_train_dir = os.path.join(output_base, \"labels\", \"train\")\n",
    "labels_val_dir = os.path.join(output_base, \"labels\", \"val\")\n",
    "\n",
    "# Creating directories\n",
    "for d in [images_train_dir, images_val_dir, labels_train_dir, labels_val_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Getting all image names\n",
    "all_images = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(all_images)\n",
    "split_idx = int(len(all_images) * 0.9)\n",
    "train_imgs = all_images[:split_idx]\n",
    "val_imgs = all_images[split_idx:]\n",
    "\n",
    "def move_files(img_list, img_dest, label_dest):\n",
    "    for img_name in img_list:\n",
    "        # Move image\n",
    "        shutil.copy(os.path.join(data_dir, img_name), os.path.join(img_dest, img_name))\n",
    "        \n",
    "        # Move label\n",
    "        label_name = img_name.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(labels_dir, label_name), os.path.join(label_dest, label_name))\n",
    "\n",
    "# Moving train and val data\n",
    "move_files(train_imgs, images_train_dir, labels_train_dir)\n",
    "move_files(val_imgs, images_val_dir, labels_val_dir)\n",
    "\n",
    "print(f\"Dataset organized: {len(train_imgs)} train images, {len(val_imgs)} val images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb0903-9634-401b-8b12-9ce16a22c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.163 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.162  Python-3.11.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_dataset\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\Deepak Dhinwa\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 755k/755k [00:00<00:00, 1.38MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5.35M/5.35M [00:01<00:00, 3.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 156.5146.4 MB/s, size: 90.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yol\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_dataset\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Training the model\n",
    "results = model.train(\n",
    "    data=r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_dataset\\data.yaml\",\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0 \n",
    ")\n",
    "\n",
    "\n",
    "model.save(\"number_plate_detector_yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2dc983-dc02-4a43-a819-83b321ceb13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.163 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.162  Python-3.11.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_dataset\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.2 ms, read: 816.8824.0 MB/s, size: 90.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yol\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 604.9298.6 MB/s, size: 79.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50     0.646G      4.802      7.408       3.72          5        640: 100%|██████████| 50/50 [00:09<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50     0.646G      3.812      3.838      2.462          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50     0.646G      3.273      3.633      2.341          4        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23    0.00234     0.0435    0.00124   0.000494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50     0.646G      3.345      3.419      2.168          4        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.104      0.087     0.0268     0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50     0.646G      3.007      3.185      2.014          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23    0.00972      0.217     0.0133    0.00365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50     0.646G      2.848      2.964      1.846          5        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.142       0.13     0.0906      0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50     0.646G      2.717      2.675      1.916          4        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23   0.000276     0.0435   8.94e-05   5.35e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50     0.646G      2.595      2.508      1.692          8        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.573      0.261      0.301      0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50     0.646G      2.429      2.275      1.682          4        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.576      0.087     0.0892     0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50     0.646G       2.32      2.117      1.632          4        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.724      0.348      0.386      0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50     0.646G      2.367      2.071      1.595          6        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.237      0.217      0.174     0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50     0.646G      2.281      2.002      1.607          9        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.41      0.609      0.471      0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50     0.646G      2.156       1.86      1.498          7        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.492      0.391      0.445      0.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50     0.646G      2.062      1.723      1.495          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.732      0.239       0.39      0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50     0.646G          2      1.736      1.458          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.69      0.522      0.539      0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50     0.646G      1.947      1.679      1.456          5        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.559      0.565      0.595      0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50     0.646G      1.895      1.598      1.427          5        640: 100%|██████████| 50/50 [00:10<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.52      0.304      0.335      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50     0.646G       2.07      1.767      1.493          9        640: 100%|██████████| 50/50 [00:08<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.874      0.565       0.68      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50     0.646G      1.853      1.521      1.392          5        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.608      0.777      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50     0.646G      1.792      1.485      1.405          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.631      0.609      0.585      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50     0.646G      1.789      1.474      1.367          7        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.891      0.609      0.713      0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50     0.646G       1.75      1.424      1.361          6        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.792      0.652      0.697      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50     0.646G      1.669      1.433      1.338          9        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.558      0.348        0.4      0.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50     0.646G      1.686      1.302      1.283          5        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.903      0.696      0.835      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50     0.646G      1.707      1.333      1.364          6        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.881      0.652       0.79      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50     0.646G       1.62      1.256      1.255          6        640: 100%|██████████| 50/50 [00:09<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.876      0.612      0.771      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50     0.646G      1.519      1.197      1.219          8        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.641      0.773      0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50     0.646G      1.663      1.211       1.25          9        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.79      0.826      0.843      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50     0.646G      1.498      1.069      1.232          5        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.89      0.706      0.848      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50     0.646G       1.61      1.157      1.241          6        640: 100%|██████████| 50/50 [00:09<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.981      0.739      0.867      0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50     0.646G      1.541      1.122      1.226          5        640: 100%|██████████| 50/50 [00:09<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.948      0.799      0.902      0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50     0.646G      1.491      1.107       1.21         12        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.647      0.806       0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50     0.646G      1.472      1.153      1.238          9        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.994      0.652       0.81      0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50     0.646G      1.426      1.043      1.168          6        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.814      0.888      0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50     0.646G      1.401      1.011      1.147          0        640: 100%|██████████| 50/50 [00:08<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.964      0.826      0.911      0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50     0.646G      1.418      1.031      1.189          7        640: 100%|██████████| 50/50 [00:08<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.962      0.739      0.865      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50     0.646G      1.334          1      1.171          5        640: 100%|██████████| 50/50 [00:10<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.964      0.739      0.877      0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50     0.646G      1.379     0.9782       1.15          6        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.867      0.826      0.908      0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50     0.646G      1.364      1.034      1.168          4        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.912      0.907       0.94      0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50     0.646G      1.356     0.9448       1.13         10        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.928       0.87       0.94      0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50     0.646G      1.194     0.9214      1.114          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23       0.94      0.739      0.915      0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50     0.646G      1.243     0.9124      1.131          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.872      0.913      0.925      0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50     0.646G      1.231      0.893       1.14          3        640: 100%|██████████| 50/50 [00:10<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.871       0.87      0.928      0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50     0.646G      1.186     0.8603      1.099          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.772      0.925      0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50     0.646G      1.201     0.8919      1.098          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.951       0.85       0.93      0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50     0.646G      1.154     0.8581      1.105          3        640: 100%|██████████| 50/50 [00:08<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23          1      0.849      0.945      0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50     0.646G      1.131     0.8192      1.094          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.967       0.87      0.953      0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50     0.646G      1.184     0.8289      1.123          3        640: 100%|██████████| 50/50 [00:09<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.955      0.929      0.955      0.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50     0.646G      1.151      0.791      1.085          3        640: 100%|██████████| 50/50 [00:08<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.956      0.957       0.96       0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50     0.646G      1.099     0.7881      1.089          3        640: 100%|██████████| 50/50 [00:07<00:00,  6.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.962      0.913      0.958      0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.127 hours.\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train3\\weights\\best.pt...\n",
      "Ultralytics 8.3.162  Python-3.11.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         23         23      0.962      0.913      0.958      0.668\n",
      "Speed: 0.3ms preprocess, 3.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "\n",
    "results = model.train(\n",
    "    data=r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\yolo_dataset\\data.yaml\",  \n",
    "    epochs=50,       \n",
    "    imgsz=640,       \n",
    "    batch=4,         \n",
    "    device=0,       \n",
    "    workers=0,      \n",
    "    optimizer='AdamW',  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.save(\"number_plate_detector_yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef51cd8-37b3-44e6-90fc-e1527b33aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg: 384x640 1 number_plate, 35.2ms\n",
      "Speed: 3.1ms preprocess, 35.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train3/weights/best.pt\")\n",
    "\n",
    "# Testing on a sample image\n",
    "img_path = r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg\"\n",
    "results = model(img_path, show=True)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5845f4e8-38c3-42d2-be94-934416de3143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg: 384x640 1 number_plate, 13.9ms\n",
      "Speed: 2.6ms preprocess, 13.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Detected Plate Text: YYGX\n"
     ]
    }
   ],
   "source": [
    "# Loading models\n",
    "plate_detector = YOLO(\"runs/detect/train3/weights/best.pt\")\n",
    "processor = TrOCRProcessor.from_pretrained('./trocr_numberplate_finetuned', use_fast=True)\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('./trocr_numberplate_finetuned').to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Detecting plate\n",
    "img_path = r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\number plate detector data\\0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg\"\n",
    "results = plate_detector(img_path)\n",
    "\n",
    "# Reading plate text\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    plate_img = Image.open(img_path).crop((x1, y1, x2, y2))\n",
    "    \n",
    "    pixel_values = processor(images=plate_img, return_tensors=\"pt\").pixel_values.to(trocr_model.device)\n",
    "    generated_ids = trocr_model.generate(pixel_values)\n",
    "    plate_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"Detected Plate Text: {plate_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312562c4-0123-43cb-b8cc-067b04ee8f7f",
   "metadata": {},
   "source": [
    "Final validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c9829be-7e3f-424e-aa3f-76f1d3ed3b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 13.5ms\n",
      "Speed: 2.2ms preprocess, 13.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 (no detections), 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x640 (no detections), 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 (no detections), 12.2ms\n",
      "Speed: 1.9ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 640x608 (no detections), 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 352x640 (no detections), 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x608 (no detections), 14.6ms\n",
      "Speed: 2.9ms preprocess, 14.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 (no detections), 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 (no detections), 13.5ms\n",
      "Speed: 4.5ms preprocess, 13.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 (no detections), 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 384x640 (no detections), 16.9ms\n",
      "Speed: 2.5ms preprocess, 16.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 256x640 (no detections), 13.5ms\n",
      "Speed: 1.7ms preprocess, 13.5ms inference, 0.8ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 640x608 (no detections), 12.8ms\n",
      "Speed: 4.1ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 608x640 1 number_plate, 16.4ms\n",
      "Speed: 3.0ms preprocess, 16.4ms inference, 2.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 512x640 1 number_plate, 20.0ms\n",
      "Speed: 1.9ms preprocess, 20.0ms inference, 2.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 608x640 (no detections), 18.2ms\n",
      "Speed: 3.6ms preprocess, 18.2ms inference, 1.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 (no detections), 13.8ms\n",
      "Speed: 1.6ms preprocess, 13.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 192x640 (no detections), 16.1ms\n",
      "Speed: 1.5ms preprocess, 16.1ms inference, 1.2ms postprocess per image at shape (1, 3, 192, 640)\n",
      "\n",
      "0: 544x640 1 number_plate, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 (no detections), 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 2 number_plates, 11.3ms\n",
      "Speed: 3.9ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 544x640 (no detections), 17.7ms\n",
      "Speed: 2.4ms preprocess, 17.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 608x640 1 number_plate, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 15.4ms\n",
      "Speed: 3.1ms preprocess, 15.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 (no detections), 10.9ms\n",
      "Speed: 3.6ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 (no detections), 10.1ms\n",
      "Speed: 2.9ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x608 (no detections), 11.6ms\n",
      "Speed: 3.4ms preprocess, 11.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 576x640 (no detections), 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.6ms\n",
      "Speed: 4.1ms preprocess, 15.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 (no detections), 12.0ms\n",
      "Speed: 1.8ms preprocess, 12.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 640x608 (no detections), 9.8ms\n",
      "Speed: 3.0ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 352x640 (no detections), 11.9ms\n",
      "Speed: 1.7ms preprocess, 11.9ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x608 (no detections), 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 number_plate, 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 (no detections), 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 288x640 (no detections), 11.9ms\n",
      "Speed: 1.3ms preprocess, 11.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 384x640 (no detections), 12.5ms\n",
      "Speed: 2.1ms preprocess, 12.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 256x640 (no detections), 12.8ms\n",
      "Speed: 1.6ms preprocess, 12.8ms inference, 0.9ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 640x640 (no detections), 11.2ms\n",
      "Speed: 3.1ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 1 number_plate, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 2.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 512x640 (no detections), 15.6ms\n",
      "Speed: 2.3ms preprocess, 15.6ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 576x640 (no detections), 13.7ms\n",
      "Speed: 2.8ms preprocess, 13.7ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 (no detections), 12.4ms\n",
      "Speed: 1.7ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 192x640 (no detections), 13.1ms\n",
      "Speed: 1.2ms preprocess, 13.1ms inference, 0.8ms postprocess per image at shape (1, 3, 192, 640)\n",
      "\n",
      "0: 544x640 1 number_plate, 14.3ms\n",
      "Speed: 3.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 2 number_plates, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 608x640 2 number_plates, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 2.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 544x640 (no detections), 14.1ms\n",
      "Speed: 2.5ms preprocess, 14.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 608x640 (no detections), 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    }
   ],
   "source": [
    "# Loading models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vehicle_model = YOLO(\"yolov8n.pt\")\n",
    "plate_model = YOLO(\"number_plate_detector_yolov8n.pt\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('./trocr_numberplate_finetuned')\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('./trocr_numberplate_finetuned').to(device)\n",
    "\n",
    "tracker = Sort()\n",
    "\n",
    "# Speed Calculation\n",
    "def estimate_speed(p1, p2, fps, ppm=8.8):\n",
    "    dist_pixels = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    dist_meters = dist_pixels / ppm\n",
    "    speed = (dist_meters * fps) * 3.6\n",
    "    return speed\n",
    "\n",
    "cap = cv2.VideoCapture(r\"D:\\projects\\Automated Vehicle Speed & Number Plate Detection System\\data\\videos\\2103099-uhd_3840_2160_30fps.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "vehicle_positions = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = vehicle_model(frame)\n",
    "    dets = []\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls[0].cpu().numpy())\n",
    "        if cls_id in [2, 3, 5, 7]:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            dets.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "    dets = np.array(dets)\n",
    "    tracks = tracker.update(dets)\n",
    "\n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2, track_id = track\n",
    "        cx = int((x1 + x2) / 2)\n",
    "        cy = int((y1 + y2) / 2)\n",
    "\n",
    "        if track_id in vehicle_positions:\n",
    "            prev_pos = vehicle_positions[track_id]\n",
    "            speed = estimate_speed(prev_pos, (cx, cy), fps)\n",
    "        else:\n",
    "            speed = 0\n",
    "\n",
    "        vehicle_positions[track_id] = (cx, cy)\n",
    "\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,255,0), 2)\n",
    "        cv2.putText(frame, f'ID:{int(track_id)} Speed:{int(speed)}km/h', (int(x1), int(y1)-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "        vehicle_crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        plate_results = plate_model(vehicle_crop)\n",
    "\n",
    "        for pbox in plate_results[0].boxes:\n",
    "            px1, py1, px2, py2 = pbox.xyxy[0].cpu().numpy()\n",
    "            plate_crop = vehicle_crop[int(py1):int(py2), int(px1):int(px2)]\n",
    "\n",
    "            plate_rgb = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2RGB)\n",
    "            inputs = processor(images=plate_rgb, return_tensors=\"pt\").to(device)\n",
    "            outputs = trocr_model.generate(**inputs)\n",
    "            plate_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "            cv2.rectangle(vehicle_crop, (int(px1), int(py1)), (int(px2), int(py2)), (255,0,0), 2)\n",
    "            cv2.putText(vehicle_crop, plate_text, (int(px1), int(py1)-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
    "\n",
    "    frame_resized = cv2.resize(frame, (1280, int(frame.shape[0] * 1280 / frame.shape[1])))\n",
    "    cv2.imshow(\"Vehicle Speed & Plate Detection\", frame_resized)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c7e40-2095-4545-a5f2-15434e3c7ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plate_detection_env",
   "language": "python",
   "name": "plate_detection_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
